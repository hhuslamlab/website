@inproceedings{wu+cotterell.acl19, 
  title = {Exact Hard Monotonic Attention for Character-Level Transduction},
  venue = {ACL},
  year = {2019},
  arXiv = {https://arxiv.org/pdf/1905.06319},
  anthology = {https://www.aclweb.org/anthology/P19-1148.pdf},
  author = {Wu, Shijie and 
	Cotterell, Ryan},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month = {July},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  pages = {1530--1537},
  abstract = {Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.},
  url = {https://www.aclweb.org/anthology/P19-1148.pdf},
}
